import cv2
import os
import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image
from collections import deque, Counter 
from tqdm import tqdm

# ==========================================
# 1. è¨­å®šåƒæ•¸èˆ‡è·¯å¾‘
# ==========================================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_PATH = 'convnext_best.pth'
VIDEO_PATH = './vlog.mp4'
RESULTS_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'results'))
os.makedirs(RESULTS_DIR, exist_ok=True)
OUTPUT_PATH = os.path.join(RESULTS_DIR, 'output_result.mp4')
print(OUTPUT_PATH)

# åƒæ•¸è¨­å®š
WINDOW_SIZE = 12  # å¹³æ»‘çª—å£ï¼šçœ‹éå» 12 å¼µåœ–ä¾†æŠ•ç¥¨ (æ•¸å­—è¶Šå¤§è¶Šç©©ï¼Œä½†åæ‡‰è¶Šæ…¢)
emotion_history = deque(maxlen=WINDOW_SIZE) # å„²å­˜æœ€è¿‘ N æ¬¡çš„æƒ…ç·’
confidence_history = deque(maxlen=WINDOW_SIZE) # å„²å­˜æœ€è¿‘ N æ¬¡çš„ä¿¡å¿ƒåº¦

LABEL_MAP_CODE = {
    'a': 'neutral', 'b': 'happy', 'c': 'sad', 'd': 'angry',
    'e': 'disgust', 'f': 'fear', 'g': 'surprise'
}
CLASSES = list(LABEL_MAP_CODE.values())
IDX_TO_CLASS = {i: label for i, label in enumerate(CLASSES)}

# ==========================================
# 2. è¼‰å…¥æ¨¡å‹
# ==========================================
def build_convnext(num_classes):
    model = models.convnext_base(weights=None)
    in_features = model.classifier[2].in_features
    model.classifier[2] = nn.Linear(in_features, num_classes)
    return model

print("æ­£åœ¨è¼‰å…¥æ¨¡å‹...")
model = build_convnext(len(CLASSES))
try:
    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")
except FileNotFoundError:
    print(f"âŒ æ‰¾ä¸åˆ°æ¬Šé‡æª”: {MODEL_PATH}")
    exit()

model.to(DEVICE)
model.eval()

inference_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# ==========================================
# 3. è™•ç†å½±ç‰‡ä¸»ç¨‹åº
# ==========================================
cap = cv2.VideoCapture(VIDEO_PATH)

if not cap.isOpened():
    print(f"âŒ ç„¡æ³•é–‹å•Ÿå½±ç‰‡: {VIDEO_PATH}ï¼Œè«‹ç¢ºèªè·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚")
    exit()

# å–å¾—å½±ç‰‡è³‡è¨Š (å¯¬ã€é«˜ã€FPS)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
print(f"å½±ç‰‡è³‡è¨Š: {width}x{height}, FPS: {fps}, ç¸½å¹€æ•¸: {total_frames}")

fourcc = cv2.VideoWriter_fourcc(*'mp4v') 
out = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))
if not out.isOpened():
    print(f"âŒ ç„¡æ³•å»ºç«‹å½±ç‰‡æª”æ¡ˆ: {OUTPUT_PATH}ï¼Œè«‹æª¢æŸ¥è·¯å¾‘æˆ–æ¬Šé™ã€‚")
    cap.release()
    cv2.destroyAllWindows()
    exit()
print("ğŸš€ é–‹å§‹åˆ†æå½±ç‰‡... (é€™å¯èƒ½éœ€è¦ä¸€é»æ™‚é–“)")

for _ in tqdm(range(total_frames)):
    ret, frame = cap.read()
    if not ret: break

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)

    if len(faces) > 0:
        # æ‰¾å‡ºé¢ç©æœ€å¤§çš„è‡‰ (w * h)
        target_face = max(faces, key=lambda f: f[2] * f[3])
        (x, y, w, h) = target_face
        
        # ç•«æ¡†
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 3)
        
        try:
            # é æ¸¬
            face_img = frame[y:y+h, x:x+w]
            pil_img = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))
            input_tensor = inference_transform(pil_img).unsqueeze(0).to(DEVICE)
            
            with torch.no_grad():
                outputs = model(input_tensor)
                probs = torch.nn.functional.softmax(outputs, dim=1)
                conf, preds = torch.max(probs, 1)
                
                current_emotion = IDX_TO_CLASS[preds.item()]
                current_conf = conf.item()

            # å¹³æ»‘åŒ–è™•ç†é‚è¼¯
            emotion_history.append(current_emotion)
            confidence_history.append(current_conf)
            
            # 1. æŠ•ç¥¨æ±ºå®šé¡¯ç¤ºå“ªå€‹æƒ…ç·’ (Mode)
            # ä¾‹å¦‚: [Happy, Happy, Neutral, Happy, Happy] -> é¡¯ç¤º Happy
            most_common_emotion = Counter(emotion_history).most_common(1)[0][0]
            avg_conf = sum(confidence_history) / len(confidence_history)
            label_text = f"{most_common_emotion} ({avg_conf:.0%})"
            color = (0, 255, 0)
            if most_common_emotion in ['angry', 'fear', 'sad', 'disgust']:
                color = (0, 0, 255) # ç´…è‰²
            elif most_common_emotion == 'neutral':
                color = (255, 255, 0) # é»ƒè‰²
            
            cv2.putText(frame, label_text, (x, y-15), 
                        cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)
            
        except Exception:
            pass
    else:
        # å¦‚æœæ²’åµæ¸¬åˆ°è‡‰ï¼Œæ¸…ç©ºæ­·å²ï¼Œä»¥å…ä¸‹æ¬¡ä¸€åµæ¸¬åˆ°å°±é¡¯ç¤ºèˆŠçš„
        if len(emotion_history) > 0:
            emotion_history.clear()
            confidence_history.clear()
    
    # å¯«å…¥è™•ç†å¾Œçš„ç•«é¢åˆ°æ–°å½±ç‰‡
    out.write(frame)

    cv2.imshow('Real-time Emotion (Smoothed)', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'): break

cap.release()
out.release()
cv2.destroyAllWindows()

print(f"\nâœ… åˆ†æå®Œæˆï¼çµæœå·²å„²å­˜ç‚º: {os.path.join(RESULTS_DIR, 'output_result.mp4')}")